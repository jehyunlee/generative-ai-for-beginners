{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 41, 20089, 374, 279, 18172, 11841, 505, 279, 8219, 323, 279, 7928, 304, 279, 25450, 744, 13, 1102, 374, 264, 6962, 14880, 449, 264, 3148, 832, 7716, 52949, 339, 430, 315, 279, 8219, 11, 719, 1403, 9976, 7561, 34902, 3115, 430, 315, 682, 279, 1023, 33975, 304, 279, 25450, 744, 11093, 13, 50789, 374, 832, 315, 279, 72021, 6302, 9621, 311, 279, 19557, 8071, 304, 279, 3814, 13180, 11, 323, 706, 1027, 3967, 311, 14154, 86569, 2533, 1603, 12715, 3925, 13, 1102, 374, 7086, 1306, 279, 13041, 10087, 50789, 8032, 777, 60, 3277, 19894, 505, 9420, 11, 50789, 649, 387, 10107, 3403, 369, 1202, 27000, 3177, 311, 6445, 9621, 35612, 17706, 508, 60, 323, 374, 389, 5578, 279, 4948, 1481, 1315, 478, 5933, 1665, 304, 279, 3814, 13180, 1306, 279, 17781, 323, 50076, 627]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'\\n',\n",
       " b'J',\n",
       " b'upiter',\n",
       " b' is',\n",
       " b' the',\n",
       " b' fifth',\n",
       " b' planet',\n",
       " b' from',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b' and',\n",
       " b' the',\n",
       " b' largest',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' a',\n",
       " b' gas',\n",
       " b' giant',\n",
       " b' with',\n",
       " b' a',\n",
       " b' mass',\n",
       " b' one',\n",
       " b'-th',\n",
       " b'ousand',\n",
       " b'th',\n",
       " b' that',\n",
       " b' of',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b',',\n",
       " b' but',\n",
       " b' two',\n",
       " b'-and',\n",
       " b'-a',\n",
       " b'-half',\n",
       " b' times',\n",
       " b' that',\n",
       " b' of',\n",
       " b' all',\n",
       " b' the',\n",
       " b' other',\n",
       " b' planets',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b' combined',\n",
       " b'.',\n",
       " b' Jupiter',\n",
       " b' is',\n",
       " b' one',\n",
       " b' of',\n",
       " b' the',\n",
       " b' brightest',\n",
       " b' objects',\n",
       " b' visible',\n",
       " b' to',\n",
       " b' the',\n",
       " b' naked',\n",
       " b' eye',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b',',\n",
       " b' and',\n",
       " b' has',\n",
       " b' been',\n",
       " b' known',\n",
       " b' to',\n",
       " b' ancient',\n",
       " b' civilizations',\n",
       " b' since',\n",
       " b' before',\n",
       " b' recorded',\n",
       " b' history',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' named',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Roman',\n",
       " b' god',\n",
       " b' Jupiter',\n",
       " b'.[',\n",
       " b'19',\n",
       " b']',\n",
       " b' When',\n",
       " b' viewed',\n",
       " b' from',\n",
       " b' Earth',\n",
       " b',',\n",
       " b' Jupiter',\n",
       " b' can',\n",
       " b' be',\n",
       " b' bright',\n",
       " b' enough',\n",
       " b' for',\n",
       " b' its',\n",
       " b' reflected',\n",
       " b' light',\n",
       " b' to',\n",
       " b' cast',\n",
       " b' visible',\n",
       " b' shadows',\n",
       " b',[',\n",
       " b'20',\n",
       " b']',\n",
       " b' and',\n",
       " b' is',\n",
       " b' on',\n",
       " b' average',\n",
       " b' the',\n",
       " b' third',\n",
       " b'-b',\n",
       " b'right',\n",
       " b'est',\n",
       " b' natural',\n",
       " b' object',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Moon',\n",
       " b' and',\n",
       " b' Venus',\n",
       " b'.\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "_AmbiguousModuleClientUsageError",
     "evalue": "Ambiguous use of module client; please set `openai.api_type` or the `OPENAI_API_TYPE` environment variable to `openai` or `azure`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_AmbiguousModuleClientUsageError\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m## 3. Run the prompt\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion\u001b[39m(prompt, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     15\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m---> 16\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     17\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     19\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m# this is the degree of randomness of the model's output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_utils/_proxy.py:25\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_utils/_proxy.py:61\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_cache:\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__proxied\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proxied \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_module_client.py:12\u001b[0m, in \u001b[0;36mChatProxy.__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resources\u001b[38;5;241m.\u001b[39mChat:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchat\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/__init__.py:276\u001b[0m, in \u001b[0;36m_load_client\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m has_azure_ad \u001b[38;5;241m=\u001b[39m _has_azure_ad_credentials()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_openai \u001b[38;5;129;01mand\u001b[39;00m (has_azure \u001b[38;5;129;01mor\u001b[39;00m has_azure_ad):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _AmbiguousModuleClientUsageError()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (azure_ad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m azure_ad_token_provider \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _AmbiguousModuleClientUsageError()\n",
      "\u001b[0;31m_AmbiguousModuleClientUsageError\u001b[0m: Ambiguous use of module client; please set `openai.api_type` or the `OPENAI_API_TYPE` environment variable to `openai` or `azure`"
     ]
    }
   ],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Martian War of 2076 - A Lesson Plan\n",
      "\n",
      "Objective: \n",
      "To educate students about the Martian War of 2076, its causes, key events, and consequences, fostering critical thinking, historical analysis, and empathy.\n",
      "\n",
      "Grade Level: \n",
      "High School (9th-12th grade)\n",
      "\n",
      "Duration: \n",
      "2-3 class periods (approximately 90 minutes each)\n",
      "\n",
      "Materials Needed:\n",
      "1. Access to research materials (books, articles, online resources)\n",
      "2. Whiteboard or blackboard\n",
      "3. Markers or chalk\n",
      "4. Handouts (optional)\n",
      "5. Multimedia resources (optional)\n",
      "\n",
      "Lesson Plan:\n",
      "\n",
      "Introduction (10 minutes):\n",
      "1. Begin the lesson by asking students if they have heard about the Martian War of 2076. Encourage them to share any prior knowledge or assumptions they may have.\n",
      "2. Explain the objective of the lesson and its relevance in understanding historical conflicts and their impact on society.\n",
      "\n",
      "Background and Context (20 minutes):\n",
      "1. Provide a brief overview of the colonization of Mars and the tensions that arose between Earth and Mars leading up to the war.\n",
      "2. Discuss the political, economic, and environmental factors that contributed to the conflict.\n",
      "3. Highlight the role of technological advancements and their impact on warfare during this period.\n",
      "\n",
      "Causes and Triggers (30 minutes):\n",
      "1. Divide the class into small groups and assign each group a specific cause or trigger of the Martian War.\n",
      "2. Instruct the groups to research and discuss their assigned cause or trigger, including its significance and how it contributed to the outbreak of war.\n",
      "3. Have each group present their findings to the class, encouraging discussion and debate.\n",
      "\n",
      "Key Events and Battles (40 minutes):\n",
      "1. Provide an overview of the major events and battles that took place during the Martian War.\n",
      "2. Use visual aids, maps, or multimedia resources to enhance understanding and engagement.\n",
      "3. Discuss the strategies, tactics, and weaponry used by both sides.\n",
      "4. Encourage students to analyze the impact of these events on the outcome of the war.\n",
      "\n",
      "Consequences and Aftermath (30 minutes):\n",
      "1. Discuss the consequences of the Martian War on both Earth and Mars, including political, social, and economic ramifications.\n",
      "2. Explore the impact on the environment and the subsequent efforts for reconstruction and reconciliation.\n",
      "3. Facilitate a class discussion on the lessons learned from the Martian War and how they can be applied to prevent future conflicts.\n",
      "\n",
      "Reflection and Empathy (20 minutes):\n",
      "1. Assign students to write a short reflection on the Martian War, focusing on the perspectives of individuals affected by the conflict.\n",
      "2. Encourage students to empathize with the experiences of soldiers, civilians, and leaders on both sides.\n",
      "3. Discuss the importance of empathy in understanding historical events and fostering peace.\n",
      "\n",
      "Conclusion (10 minutes):\n",
      "1. Summarize the key points discussed throughout the lesson.\n",
      "2. Allow time for any remaining questions or discussions.\n",
      "3. Emphasize the significance of studying historical conflicts to gain insights into human nature, diplomacy, and the importance of peaceful resolutions.\n",
      "\n",
      "Extension Activities (optional):\n",
      "1. Ask students to create a timeline of the Martian War, including key events and their significance.\n",
      "2. Organize a debate or mock trial, where students can argue for or against the actions of specific individuals or nations during the war.\n",
      "3. Assign a creative project, such as writing a short story or creating artwork, that explores the experiences of individuals during the Martian War.\n",
      "\n",
      "Note: Adjust the duration and complexity of the lesson plan based on the grade level and the students' prior knowledge and abilities.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is a really big planet that is fifth from the Sun. It is made of gas and is the largest planet in our Solar System. It is much smaller than the Sun, but much bigger than all the other planets combined. People have known about Jupiter for a really long time because it is very bright in the night sky. It is named after a god from ancient Rome. Sometimes, Jupiter is so bright that it can make shadows on Earth. It is usually the third-brightest thing we can see at night, after the Moon and Venus.\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "System sets assistant context\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, you know, just a little place called Globe Life Field in Arlington, Texas. They just wanted to change it up from the usual location.\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
